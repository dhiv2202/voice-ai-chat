<script>
let mediaRecorder;
let audioChunks = [];
let silenceTimer;
let audioContext;
let analyser;
let source;

async function startRecording() {
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  audioChunks = [];

  mediaRecorder = new MediaRecorder(stream);
  mediaRecorder.ondataavailable = (e) => audioChunks.push(e.data);

  mediaRecorder.onstop = async () => {
    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
    const formData = new FormData();
    formData.append("audio", audioBlob, "recording.webm");

    const response = await fetch("/transcribe", {
      method: "POST",
      body: formData,
    });

    const audio = document.getElementById("responseAudio");
    const audioBlobResp = await response.blob();
    audio.src = URL.createObjectURL(audioBlobResp);
    audio.play();
  };

  mediaRecorder.start();
  monitorSilence(stream);
}

function monitorSilence(stream) {
  audioContext = new (window.AudioContext || window.webkitAudioContext)();
  source = audioContext.createMediaStreamSource(stream);
  analyser = audioContext.createAnalyser();
  source.connect(analyser);
  const data = new Uint8Array(analyser.fftSize);

  const checkSilence = () => {
    analyser.getByteTimeDomainData(data);
    let max = 0;
    for (let i = 0; i < data.length; i++) {
      const deviation = Math.abs(data[i] - 128);
      if (deviation > max) max = deviation;
    }

    if (max < 10) {
      if (!silenceTimer) {
        silenceTimer = setTimeout(() => {
          stopRecording();
        }, 1000); // 1 second of silence = stop
      }
    } else {
      clearTimeout(silenceTimer);
      silenceTimer = null;
    }

    requestAnimationFrame(checkSilence);
  };

  checkSilence();
}

function stopRecording() {
  if (mediaRecorder && mediaRecorder.state !== "inactive") {
    mediaRecorder.stop();
    if (audioContext) audioContext.close();
  }
}
</script>
